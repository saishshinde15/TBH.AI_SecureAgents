{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBH Secure Agents - Guardrails Example\n",
    "\n",
    "This notebook demonstrates how to use the `guardrails` parameter with the TBH Secure Agents framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the latest version of the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tbh-secure-agents in ./build_env/lib/python3.12/site-packages (0.2.2)\n",
      "Requirement already satisfied: google-generativeai>=0.4.0 in ./build_env/lib/python3.12/site-packages (from tbh-secure-agents) (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in ./build_env/lib/python3.12/site-packages (from google-generativeai>=0.4.0->tbh-secure-agents) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in ./build_env/lib/python3.12/site-packages (from google-generativeai>=0.4.0->tbh-secure-agents) (2.25.0rc0)\n",
      "Requirement already satisfied: google-api-python-client in ./build_env/lib/python3.12/site-packages (from google-generativeai>=0.4.0->tbh-secure-agents) (2.169.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in ./build_env/lib/python3.12/site-packages (from google-generativeai>=0.4.0->tbh-secure-agents) (2.39.0)\n",
      "Requirement already satisfied: protobuf in ./build_env/lib/python3.12/site-packages (from google-generativeai>=0.4.0->tbh-secure-agents) (5.29.4)\n",
      "Requirement already satisfied: pydantic in ./build_env/lib/python3.12/site-packages (from google-generativeai>=0.4.0->tbh-secure-agents) (2.11.4)\n",
      "Requirement already satisfied: tqdm in ./build_env/lib/python3.12/site-packages (from google-generativeai>=0.4.0->tbh-secure-agents) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in ./build_env/lib/python3.12/site-packages (from google-generativeai>=0.4.0->tbh-secure-agents) (4.13.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./build_env/lib/python3.12/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.4.0->tbh-secure-agents) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in ./build_env/lib/python3.12/site-packages (from google-api-core->google-generativeai>=0.4.0->tbh-secure-agents) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in ./build_env/lib/python3.12/site-packages (from google-api-core->google-generativeai>=0.4.0->tbh-secure-agents) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in ./build_env/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.4.0->tbh-secure-agents) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in ./build_env/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.4.0->tbh-secure-agents) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./build_env/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai>=0.4.0->tbh-secure-agents) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./build_env/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai>=0.4.0->tbh-secure-agents) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./build_env/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai>=0.4.0->tbh-secure-agents) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./build_env/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai>=0.4.0->tbh-secure-agents) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./build_env/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai>=0.4.0->tbh-secure-agents) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./build_env/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai>=0.4.0->tbh-secure-agents) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./build_env/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai>=0.4.0->tbh-secure-agents) (2025.4.26)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./build_env/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai>=0.4.0->tbh-secure-agents) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in ./build_env/lib/python3.12/site-packages (from google-api-python-client->google-generativeai>=0.4.0->tbh-secure-agents) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in ./build_env/lib/python3.12/site-packages (from google-api-python-client->google-generativeai>=0.4.0->tbh-secure-agents) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./build_env/lib/python3.12/site-packages (from google-api-python-client->google-generativeai>=0.4.0->tbh-secure-agents) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in ./build_env/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.4.0->tbh-secure-agents) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./build_env/lib/python3.12/site-packages (from pydantic->google-generativeai>=0.4.0->tbh-secure-agents) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./build_env/lib/python3.12/site-packages (from pydantic->google-generativeai>=0.4.0->tbh-secure-agents) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./build_env/lib/python3.12/site-packages (from pydantic->google-generativeai>=0.4.0->tbh-secure-agents) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tbh-secure-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saish/Downloads/tbh_secure_agents/build_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tbh_secure_agents import Expert, Operation, Squad\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your API key\n",
    "\n",
    "You can either set it as an environment variable or directly in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Set as environment variable\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Option 2: Set directly in code\n",
    "api_key = \"\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experts with Template Variables\n",
    "\n",
    "Notice how we use curly braces `{}` to define template variables that will be replaced by the guardrails values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:47:45,390 - tbh_secure_agents.agent - INFO - Gemini API configured.\n",
      "2025-05-03 12:47:45,392 - tbh_secure_agents.agent - INFO - Expert 'Research Expert specializing in {topic_area}' initialized with Gemini model 'gemini-2.0-flash-lite' and security profile 'default'.\n",
      "2025-05-03 12:47:45,393 - tbh_secure_agents.agent - INFO - Gemini API configured.\n",
      "2025-05-03 12:47:45,394 - tbh_secure_agents.agent - INFO - Expert 'Content Writer' initialized with Gemini model 'gemini-2.0-flash-lite' and security profile 'default'.\n"
     ]
    }
   ],
   "source": [
    "researcher = Expert(\n",
    "    specialty=\"Research Expert specializing in {topic_area}\",\n",
    "    objective=\"Research and analyze information about {specific_topic}\",\n",
    "    background=\"You have extensive knowledge in {topic_area} research.\",\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "writer = Expert(\n",
    "    specialty=\"Content Writer\",\n",
    "    objective=\"Create engaging {content_type} based on research findings\",\n",
    "    background=\"You excel at creating {tone} content for {audience_level} audiences.\",\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Operations with Template Variables and Select Syntax\n",
    "\n",
    "We can use both simple template variables and the more advanced select syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_operation = Operation(\n",
    "    instructions=\"\"\"\n",
    "    Research the topic of {specific_topic} within the field of {topic_area}.\n",
    "    Focus on recent developments and key concepts.\n",
    "    \n",
    "    {depth, select,\n",
    "      basic:Provide a high-level overview suitable for beginners.|  \n",
    "      intermediate:Include more detailed information for those with some knowledge.|  \n",
    "      advanced:Provide in-depth analysis for experts in the field.\n",
    "    }\n",
    "    \"\"\",\n",
    "    output_format=\"A comprehensive research summary with key findings\",\n",
    "    expert=researcher\n",
    ")\n",
    "\n",
    "writing_operation = Operation(\n",
    "    instructions=\"\"\"\n",
    "    Based on the research findings, create a {content_type} about {specific_topic}.\n",
    "    \n",
    "    {tone, select,\n",
    "      formal:Use a professional, academic tone.|  \n",
    "      conversational:Use a friendly, approachable tone.|  \n",
    "      technical:Use precise technical language.\n",
    "    }\n",
    "    \n",
    "    The content should be suitable for a {audience_level} audience.\n",
    "    \"\"\",\n",
    "    output_format=\"A well-structured {content_type} with clear sections\",\n",
    "    expert=writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Squad with the Experts and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:48:01,980 - tbh_secure_agents.crew - INFO - Squad initialized with 2 experts and 2 operations. Process: sequential, Security Level: standard\n",
      "Squad created with 2 experts and 2 operations.\n"
     ]
    }
   ],
   "source": [
    "research_squad = Squad(\n",
    "    experts=[researcher, writer],\n",
    "    operations=[research_operation, writing_operation],\n",
    "    process=\"sequential\"  # Operations run in sequence\n",
    ")\n",
    "\n",
    "print(f\"Squad created with {len(research_squad.experts)} experts and {len(research_squad.operations)} operations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Guardrail Inputs\n",
    "\n",
    "These values will be used to replace the template variables in the experts' attributes and operations' instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_inputs = {\n",
    "    \"topic_area\": \"artificial intelligence\",\n",
    "    \"specific_topic\": \"large language models\",\n",
    "    \"depth\": \"intermediate\",\n",
    "    \"content_type\": \"blog post\",\n",
    "    \"tone\": \"conversational\",\n",
    "    \"audience_level\": \"general\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Squad with Guardrails\n",
    "\n",
    "Now we can deploy the squad with the guardrails parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting squad deployment with guardrails...\n",
      "2025-05-03 12:48:08,384 - tbh_secure_agents.crew - INFO - Squad deployment initiated...\n",
      "2025-05-03 12:48:08,388 - tbh_secure_agents.crew - INFO - Squad deployment with guardrails: ['topic_area', 'specific_topic', 'depth', 'content_type', 'tone', 'audience_level']\n",
      "2025-05-03 12:48:08,392 - tbh_secure_agents.crew - INFO - Operation '\n",
      "    Research the topic of {sp...' already assigned to Expert 'Research Expert specializing in {topic_area}'\n",
      "2025-05-03 12:48:08,393 - tbh_secure_agents.crew - INFO - Executing operation 1/2: '\n",
      "    Research the topic of {sp...'\n",
      "2025-05-03 12:48:08,396 - tbh_secure_agents.task - INFO - Operation '\n",
      "    Research the topic of {specific_topic} within...' starting execution by expert 'Research Expert specializing in {topic_area}'.\n",
      "2025-05-03 12:48:08,396 - tbh_secure_agents.agent - INFO - Expert 'Research Expert specializing in {topic_area}' starting operation execution: \n",
      "    Research the topic of {specific_topic} within the field of {topic_area}.\n",
      "    Focus on recent de...\n",
      "2025-05-03 12:48:16,122 - tbh_secure_agents.agent - INFO - Expert 'Research Expert specializing in {topic_area}' successfully executed operation on attempt 1.\n",
      "2025-05-03 12:48:16,127 - tbh_secure_agents.task - INFO - Operation '\n",
      "    Research the topic of {specific_topic} within...' finished execution successfully in 7.73 seconds.\n",
      "2025-05-03 12:48:16,130 - tbh_secure_agents.crew - INFO - Operation 1 completed successfully in 7.74s\n",
      "2025-05-03 12:48:16,131 - tbh_secure_agents.crew - INFO - Operation '\n",
      "    Based on the research fin...' already assigned to Expert 'Content Writer'\n",
      "2025-05-03 12:48:16,134 - tbh_secure_agents.crew - INFO - Executing operation 2/2: '\n",
      "    Based on the research fin...'\n",
      "2025-05-03 12:48:16,134 - tbh_secure_agents.task - INFO - Operation '\n",
      "    Based on the research findings, create a {con...' starting execution by expert 'Content Writer'.\n",
      "2025-05-03 12:48:16,135 - tbh_secure_agents.agent - INFO - Expert 'Content Writer' starting operation execution: \n",
      "    Based on the research findings, create a {content_type} about {specific_topic}.\n",
      "\n",
      "    {tone, sel...\n",
      "2025-05-03 12:48:23,410 - tbh_secure_agents.agent - INFO - Expert 'Content Writer' successfully executed operation on attempt 1.\n",
      "2025-05-03 12:48:23,421 - tbh_secure_agents.task - INFO - Operation '\n",
      "    Based on the research findings, create a {con...' finished execution successfully in 7.29 seconds.\n",
      "2025-05-03 12:48:23,427 - tbh_secure_agents.crew - INFO - Operation 2 completed successfully in 7.29s\n",
      "2025-05-03 12:48:23,427 - tbh_secure_agents.crew - INFO - Squad deployment finished. Execution time: 15.04s, Operations completed: 2, Operations failed: 0\n",
      "\n",
      "RESULT:\n",
      "================================================================================\n",
      "Okay, here's a blog post on Large Language Models, tailored for a general audience and maintaining a conversational tone, based on the provided research findings and guardrails:\n",
      "\n",
      "## Decoding the Code: Recent Developments and Key Concepts in Large Language Models\n",
      "\n",
      "Hey everyone! If you're reading this, chances are you've already heard the buzz around Large Language Models (LLMs) – models like GPT-3, LaMDA, and now, increasingly, GPT-4, Bard, and Claude. They're the engines behind those chatbots that write poems, answer complex questions, and even help you debug code. But what's *really* going on under the hood, and what's new in the LLM landscape? Let's break it down.\n",
      "\n",
      "**A Refresher: What *Are* LLMs Anyway?**\n",
      "\n",
      "At their core, LLMs are like super-powered text processors. They're sophisticated computer programs, built on a type of artificial neural network, trained on massive amounts of text and code. Think of it like this: they've read almost everything on the internet! This training process involves predicting the next word (or token – more on that later!) in a sequence, allowing the model to learn patterns, relationships, and nuances of human language (and code!). They achieve this through a process called **self-supervised learning**, which is a crucial aspect of their effectiveness and scalability. Basically, they teach *themselves* by analyzing vast amounts of data.\n",
      "\n",
      "**Key Concepts to Keep in Mind:**\n",
      "\n",
      "*   **Transformers:** This is the secret sauce! The transformer architecture is what makes LLMs so powerful. Imagine it like a highly intelligent editor that can understand context. Transformers use a mechanism called \"attention\" to weigh the importance of different words in a sentence when predicting the next word. This allows them to understand context much better than older models. They are responsible for the incredible advancements in LLMs we've seen over the past few years.\n",
      "*   **Scaling Laws:** Bigger is often better (but not always!). The performance of LLMs generally improves as you increase the size of the dataset they're trained on, the number of parameters (the model's \"memory\"), and the amount of computing power used for training. This has led to a race to build bigger and better models, but we're also seeing focus shift to making smaller models that can perform just as well.\n",
      "*   **Fine-tuning vs. Prompt Engineering:** LLMs are often *fine-tuned* on smaller datasets to become experts in specific areas. For example, to answer medical questions or write customer service responses. But a lot of the magic happens through *prompt engineering* – carefully crafting the input, or \"prompt,\" to get the desired output. Think of it like giving the model very specific instructions. This is becoming a really valuable skill!\n",
      "*   **Tokens vs. Words:** LLMs don't operate on whole words, exactly. Instead, they break text down into \"tokens.\" Tokens can be parts of words, whole words, or even punctuation marks. This tokenization is a fundamental part of how they process and understand text. It's like breaking down a recipe into its smallest parts so you can use those parts in different ways.\n",
      "\n",
      "**Recent Developments: What's New and Exciting?**\n",
      "\n",
      "The LLM field is buzzing with innovation. Here's a glimpse of what's new:\n",
      "\n",
      "*   **Multi-modal LLMs:** LLMs aren't just about text anymore! They're rapidly expanding to process and generate images, audio, and video. Think of AI that can analyze a video and summarize it, or generate an image based on a written description (like DALL-E or Midjourney). The possibilities are endless!\n",
      "*   **Improved Reasoning and Problem-Solving:** Researchers are constantly pushing the boundaries of what LLMs can *do*. They're working to improve the ability of models to think logically, solve complex problems, and understand nuanced instructions. Techniques like \"chain-of-thought prompting\" (where the model is guided to show its reasoning steps) are making big strides. It’s like the AI is showing its work!\n",
      "*   **Enhanced Control and Customization:** Developers are finding better ways to control what LLMs produce. This includes techniques to make models more aligned with human values, avoid generating harmful content (we call this \"alignment\"), and tailoring them for specific use cases.\n",
      "*   **Efficiency and Resource Optimization:** Training and running LLMs requires a lot of computing power. Scientists are working on making the models more efficient, to run on smaller devices. This is especially important for wider accessibility.\n",
      "\n",
      "**The Challenges Ahead:**\n",
      "\n",
      "While the progress is amazing, there are still some challenges to overcome:\n",
      "\n",
      "*   **Bias and Fairness:** LLMs are trained on data that reflects societal biases. It's crucial to address these biases to ensure fair and equitable outputs.\n",
      "*   **Hallucinations and Factuality:** LLMs can sometimes make things up, or \"hallucinate,\" producing convincing but incorrect information. Improving factuality and reliability is essential.\n",
      "*   **Explainability and Interpretability:** We often don't understand *why* an LLM produces a specific output. It's important to develop methods for building trust and making it easier to debug models.\n",
      "*   **Environmental Impact:** Training and running LLMs consumes a massive amount of energy. It's essential to develop more efficient models and infrastructure for sustainability.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Large Language Models are truly changing the world. Their capabilities are constantly expanding, and the future is bright. From creative writing to scientific breakthroughs, LLMs are going to be everywhere. There are challenges, but the research and development in this field promise even more exciting developments in the years to come. I encourage you to stay informed and explore the possibilities. What do *you* find most interesting about the world of LLMs? Share your thoughts in the comments!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting squad deployment with guardrails...\")\n",
    "result = research_squad.deploy(guardrails=guardrail_inputs)\n",
    "\n",
    "print(\"\\nRESULT:\")\n",
    "print(\"=\" * 80)\n",
    "print(result)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Different Guardrail Values\n",
    "\n",
    "Try changing the values in the guardrail_inputs dictionary to see how it affects the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Change to a different topic and depth\n",
    "new_guardrail_inputs = {\n",
    "    \"topic_area\": \"cybersecurity\",\n",
    "    \"specific_topic\": \"zero trust architecture\",\n",
    "    \"depth\": \"basic\",\n",
    "    \"content_type\": \"technical report\",\n",
    "    \"tone\": \"formal\",\n",
    "    \"audience_level\": \"professional\"\n",
    "}\n",
    "\n",
    "# Uncomment to run with new guardrails\n",
    "# result = research_squad.deploy(guardrails=new_guardrail_inputs)\n",
    "# print(\"\\nNEW RESULT:\")\n",
    "# print(\"=\" * 80)\n",
    "# print(result)\n",
    "# print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
